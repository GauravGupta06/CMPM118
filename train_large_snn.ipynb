{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102900a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all required libraries below\n",
    "%pip install numpy --quiet\n",
    "%pip install tonic --quiet\n",
    "%pip install matplotlib --quiet\n",
    "%pip install snntorch --quiet\n",
    "%pip install torch --quiet\n",
    "%pip install Lempel-Ziv-Complexity --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions as rf\n",
    "import tonic\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from lempel_ziv_complexity import lempel_ziv_complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aed32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/gauravgupta/CMPM118/data'\n",
    "w,h=64,64\n",
    "n_frames=100\n",
    "debug = False\n",
    "\n",
    "transforms = tonic.transforms.Compose([\n",
    "    tonic.transforms.Denoise(filter_time=10000), # removes outlier events with inactive surrounding pixels for 10ms\n",
    "    tonic.transforms.Downsample(sensor_size=tonic.datasets.DVSGesture.sensor_size, target_size=(w,h)), # downsampling image\n",
    "    tonic.transforms.ToFrame(sensor_size=(w,h,2), n_time_bins=n_frames), # n_frames frames per trail\n",
    "])\n",
    "\n",
    "train2 = tonic.datasets.DVSGesture(save_to=dataset_path, transform=transforms, train=True)\n",
    "test2 = tonic.datasets.DVSGesture(save_to=dataset_path, transform=transforms, train=False)\n",
    "\n",
    "cache_root = f\"/home/gauravgupta/CMPM118/data/dvsgesture/{w}x{h}_T{n_frames}\"\n",
    "cached_train = tonic.DiskCachedDataset(train2, cache_path=f\"{cache_root}/train\")\n",
    "cached_test  = tonic.DiskCachedDataset(test2,  cache_path=f\"{cache_root}/test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcaf9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used to figure out how many fully connected neurons need to be present in the last layer. This number depends on the w and h values. \n",
    "\n",
    "test_input = torch.zeros((1, 2, w, h))  # 2 polarity channels\n",
    "x = nn.Conv2d(2, 12, 5)(test_input)\n",
    "x = nn.MaxPool2d(2)(x)\n",
    "x = nn.Conv2d(12, 32, 5)(x)\n",
    "x = nn.MaxPool2d(2)(x)\n",
    "print(\"Output shape before flatten:\", x.shape)\n",
    "print(\"Flattened size:\", x.numel())\n",
    "flattenedSize = x.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31635f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = snn.surrogate.fast_sigmoid(slope=25) # surrogate.atan()\n",
    "beta = 0.5\n",
    "\n",
    "# 12C5-MP2-32C5-MP2-800FC11 https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_7.html\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(2, 12, 5), # in_channels, out_channels, kernel_size\n",
    "    nn.MaxPool2d(2),\n",
    "    snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True),\n",
    "    nn.Conv2d(12, 32, 5),\n",
    "    nn.MaxPool2d(2),\n",
    "    snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(flattenedSize, 11),\n",
    "    snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True, output=True)\n",
    ").to(device)\n",
    "\n",
    "def forward_pass(net, data):\n",
    "    spk_rec = []\n",
    "    snn.utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "    for step in range(data.size(0)): # data.size(0) = number of time steps\n",
    "        spk_out, mem_out = net(data[step].to(device))\n",
    "        spk_rec.append(spk_out)\n",
    "    return torch.stack(spk_rec)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "test_acc_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model():\n",
    "    correct, total = 0, 0  \n",
    "    for batch, (data, targets) in enumerate(iter(test_loader)): \n",
    "        data, targets = data.to(device), targets.to(device) # [n_frames, batch, polarity, x-pos, y-pos] [batch] \n",
    "        spk_rec = forward_pass(net, data)         \n",
    "        correct += SF.accuracy_rate(spk_rec, targets) * data.shape[0]\n",
    "        total += data.shape[0]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "cnt = 0\n",
    "active_cores = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cached_train, batch_size=64, shuffle=True, num_workers = active_cores, drop_last=True, \n",
    "                                           collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "test_loader = torch.utils.data.DataLoader(cached_test, batch_size=32, shuffle=True, num_workers = active_cores, drop_last=True, \n",
    "                                          collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch, (data, targets) in enumerate(iter(train_loader)):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        # propagating one batch through the network and evaluating loss\n",
    "        spk_rec = forward_pass(net, data)\n",
    "        loss = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        acc_hist.append(acc)\n",
    "\n",
    "        if cnt % 15 == 0:\n",
    "            print(f\"Epoch {epoch}, Iteration {batch} \\nTrain Loss: {loss.item():.2f}\")\n",
    "            print(f\"Train Accuracy: {acc * 100:.2f}%\")\n",
    "            test_acc = validate_model()            \n",
    "            test_acc_hist.append(test_acc)\n",
    "            print(f\"Test Accuracy: {test_acc * 100:.2f}%\\n\")\n",
    "\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65373a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file basically takes the trained model from the call above, the acc_hist, test_acc_hist, and loss_hist, and puts it in a graph. \n",
    "# This file also saves BOTH the model and the graph along with the TRIAL NUMBER. This number is automatically updated. \n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18,4))\n",
    "\n",
    "# Plot Train Accuracy\n",
    "axes[0].plot(acc_hist)\n",
    "axes[0].set_title(\"Train Set Accuracy\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Plot Test Accuracy\n",
    "axes[1].plot(test_acc_hist)\n",
    "axes[1].set_title(\"Test Set Accuracy\")\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Plot Training Loss\n",
    "axes[2].plot(loss_hist)\n",
    "axes[2].set_title(\"Loss History\")\n",
    "axes[2].set_xlabel(\"Iteration\")\n",
    "axes[2].set_ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "# The part below automatically saves the model/accuracy graph in a unique file without you having to do anything\n",
    "\n",
    "experiment_counter_file_path = \"results/large/experiment_counter.txt\"\n",
    "with open(experiment_counter_file_path, \"r\") as f:\n",
    "        num_str = f.read().strip()\n",
    "        num = int(num_str)\n",
    "\n",
    "num += 1\n",
    "\n",
    "with open(experiment_counter_file_path, \"w\") as f:\n",
    "    f.write(str(num))\n",
    "\n",
    "model_save_path = f\"results/large/models/Large_Take{num}.pth\"\n",
    "graph_save_path = f\"results/large/graphs/Large_Take{num}.png\"\n",
    "\n",
    "torch.save(net.state_dict(), model_save_path) \n",
    "plt.savefig(graph_save_path)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
